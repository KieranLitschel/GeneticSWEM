{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDV0-KftQyYb",
    "outputId": "dd885b22-8f49-4e9d-c5e7-7fb6eca5847f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djvSchJ7_ek4"
   },
   "source": [
    "These are URLs to the training set values and labels. Note that they are likely out of date as they refresh every 24\n",
    "hours, but you can get new URLs from https://www.drivendata.org/competitions/63/genetic-engineering-attribution/data/.\n",
    "These can also be replaced with local paths of local versions of the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AKTHkNz7mM2"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://drivendata-prod.s3.amazonaws.com/data/63/public/train_values.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20201101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201101T175059Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b3930e87c598086453a329938cc488411d78ff3ec09266474a08fbb0a069b509\"\n",
    "TRAIN_LABELS_URL = \"https://drivendata-prod.s3.amazonaws.com/data/63/public/train_labels.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20201101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201101T175059Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=461e78cc0f26a1684f968c96be7dd02bc04c992333d95e681579081c06f2a999\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK_yubJCjX1w"
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 38\n",
    "INFER_BATCH_SIZE = 38\n",
    "VOCAB_SIZE = 65500\n",
    "VALIDATION_PERCENT_SPLIT = 0.1 # if you change this will need to build a new file of train_indexes\n",
    "VALIDATION_THRESHOLD = 40 # if less than this many samples in the training set for respective class, no samples from that class are included in the validation set\n",
    "TRAIN_SET_FRAC = 1 # fraction of training and validation set to use\n",
    "BASES = [\"G\",\"A\",\"T\",\"C\",\"N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zalhu5uM_pgG"
   },
   "source": [
    "This code is used to build the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFnc3m38-ZJs"
   },
   "outputs": [],
   "source": [
    "# download the features and labels\n",
    "\n",
    "train_features_file_path = tf.keras.utils.get_file(\"train_features.csv\", TRAIN_DATA_URL)\n",
    "train_features_df = pd.read_csv(train_features_file_path, index_col=\"sequence_id\")\n",
    "train_labels_file_path = tf.keras.utils.get_file(\"train_labels.csv\", TRAIN_LABELS_URL)\n",
    "train_labels_df = pd.read_csv(train_labels_file_path, index_col=\"sequence_id\")\n",
    "\n",
    "NUM_LABELS = len(train_labels_df.columns)\n",
    "\n",
    "# seperate training set into a training and validation set\n",
    "\n",
    "indexes = list(train_features_df.index)\n",
    "np.random.seed(26082020)\n",
    "np.random.shuffle(indexes)\n",
    "# ensure that the number of labels for each class in each subset are balanced\n",
    "indexes_by_class = {key:[] for key in range(NUM_LABELS)}\n",
    "for index in indexes:\n",
    "    indexes_by_class[np.argmax(train_labels_df.loc[index].values)].append(index)\n",
    "validation_indexes = []\n",
    "train_indexes = []\n",
    "for class_no in range(NUM_LABELS):\n",
    "    number_of_samples = len(indexes_by_class[class_no])\n",
    "    # if we don't want the whole training set, then at minimum we will take 2 samples (one for each subset), as long as there are at least 2\n",
    "    number_of_samples_to_take = max(int(number_of_samples*TRAIN_SET_FRAC),min(number_of_samples,2))\n",
    "    # if there are less than VALIDATION_THRESHOLD samples in a class, we include none of that class in the validation set\n",
    "    if number_of_samples_to_take > VALIDATION_THRESHOLD:\n",
    "        validation_samples = int(number_of_samples_to_take*VALIDATION_PERCENT_SPLIT)\n",
    "    else:\n",
    "        validation_samples = 0\n",
    "    for sample_no, sample in enumerate(indexes_by_class[class_no][:number_of_samples_to_take]):\n",
    "        if sample_no < validation_samples:\n",
    "            validation_indexes.append(sample)\n",
    "        else:\n",
    "            train_indexes.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8I5S7b8I49W"
   },
   "outputs": [],
   "source": [
    "# train the BPE encoder, this takes about 15 minutes\n",
    "open(\"train_sequences.txt\",\"w\").write(\"\\n\".join(train_features_df.loc[train_indexes][\"sequence\"].values))\n",
    "spm.SentencePieceTrainer.train(input='train_sequences.txt', model_prefix='GEA_SWEM_encoder', vocab_size=VOCAB_SIZE, model_type=\"bpe\", bos_id=-1, eos_id=-1, pad_id=0, unk_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4_VCHQAQ5aV"
   },
   "outputs": [],
   "source": [
    "# encode the sequences using the trained BPE encoder\n",
    "\n",
    "encoder = spm.SentencePieceProcessor(model_file='GEA_SWEM_encoder.model')\n",
    "\n",
    "def encode_sequence(features_df, encoder):\n",
    "    # if the len(sequence)%N != 0, we discard of the extra characters, we also encode each sequence of N characters seperately as SubwordTextEncoder computes overlapping encodings\n",
    "    # only keep unique sequences\n",
    "    new_sequence_column = []\n",
    "    for sequence in tqdm(features_df[\"sequence\"]):\n",
    "        # 2 byte int works for vocab up to 65,500 in size, casting it as np.uint16 halves the memory requirements, allowing us to have a large vocabulary\n",
    "        sequence_encoded = encoder.encode(sequence)\n",
    "        # get unique encodings whilst preserving the order they occured in\n",
    "        indexes = np.unique(sequence_encoded, return_index=True)[1]\n",
    "        sequence_encoded = np.array([sequence_encoded[index] for index in sorted(indexes)],dtype=np.uint16)\n",
    "        new_sequence_column.append(sequence_encoded)\n",
    "    features_df[\"sequence\"] = new_sequence_column\n",
    "    return features_df\n",
    "\n",
    "train_features_df = encode_sequence(train_features_df, encoder)\n",
    "\n",
    "# convert one-hot features to int\n",
    "column_type_dict = {\"sequence\":object}\n",
    "for column in train_features_df.columns[1:]:\n",
    "    column_type_dict[column] = np.int16\n",
    "train_features_df = train_features_df.astype(column_type_dict)\n",
    "train_features_df.to_pickle(\"bpe_{}_encoded_train_features_df.pickle\".format(VOCAB_SIZE))\n",
    "\n",
    "# shuffle again so indexes are not ordered by class\n",
    "np.random.seed(27082020)\n",
    "np.random.shuffle(validation_indexes)\n",
    "np.random.seed(28082020)\n",
    "np.random.shuffle(train_indexes)\n",
    "# set up their dataframes\n",
    "validation_features_df = train_features_df.loc[validation_indexes]\n",
    "validation_labels_df = train_labels_df.loc[validation_indexes]\n",
    "train_features_df = train_features_df.loc[train_indexes]\n",
    "train_labels_df = train_labels_df.loc[train_indexes]\n",
    "\n",
    "# the only way to get uneven lists into tf.data.Dataset is using ragged tensors, but padded\n",
    "# batch does not support ragged tensors, and we can not pad before training as we will run out\n",
    "# of memory, so we just convert the lists to binary and then convert them back to ints in the\n",
    "# pipeline\n",
    "\n",
    "train_features_df[\"sequence\"] = [pickle.dumps(sequence) for sequence in train_features_df[\"sequence\"]]\n",
    "validation_features_df[\"sequence\"] = [pickle.dumps(sequence) for sequence in validation_features_df[\"sequence\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwr5hLnGcyft"
   },
   "outputs": [],
   "source": [
    "# build datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":train_features_df[\"sequence\"].values,\"other_features\":train_features_df.drop(columns=\"sequence\").values},train_labels_df.values))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":validation_features_df[\"sequence\"].values,\"other_features\":validation_features_df.drop(columns=\"sequence\").values},validation_labels_df.values))\n",
    "\n",
    "# save unshufled train dataset for evaluation\n",
    "unshuffled_train_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":train_features_df[\"sequence\"].values,\"other_features\":train_features_df.drop(columns=\"sequence\").values},train_labels_df.values))\n",
    "\n",
    "# shuffle train\n",
    "train_dataset = train_dataset.shuffle(len(train_features_df))\n",
    "\n",
    "# convert binary to ints\n",
    "\n",
    "def bin_to_int(sequence_tensor):\n",
    "    sequence = pickle.loads(sequence_tensor.numpy())\n",
    "    return sequence\n",
    "\n",
    "def tf_bin_to_int(*tensors):\n",
    "    if len(tensors) == 2:\n",
    "        features_dict, labels_tensor = tensors\n",
    "    else:\n",
    "        features_dict = tensors[0]\n",
    "    sequence_tensor = features_dict[\"sequence\"]\n",
    "    sequence_tensor = tf.py_function(bin_to_int, inp=[sequence_tensor], Tout=tf.int32)\n",
    "    sequence_tensor.set_shape([None])\n",
    "    features_dict[\"sequence\"] = sequence_tensor\n",
    "    if len(tensors) == 2:\n",
    "        tensors = (features_dict, labels_tensor)\n",
    "    else:\n",
    "        tensors = features_dict\n",
    "    return tensors\n",
    "\n",
    "train_dataset = train_dataset.map(tf_bin_to_int,\n",
    "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
    "unshuffled_train_dataset = unshuffled_train_dataset.map(tf_bin_to_int,\n",
    "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
    "validation_dataset = validation_dataset.map(tf_bin_to_int,\n",
    "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
    "\n",
    "# pre fetch\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "unshuffled_train_dataset = unshuffled_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# batch datasets\n",
    "train_dataset = train_dataset.padded_batch(TRAIN_BATCH_SIZE, padded_shapes=({\"sequence\": [None], \"other_features\": [None]},[None]))\n",
    "unshuffled_train_dataset = unshuffled_train_dataset.padded_batch(INFER_BATCH_SIZE, padded_shapes=({\"sequence\": [None], \"other_features\": [None]},[None]))\n",
    "validation_dataset = validation_dataset.padded_batch(INFER_BATCH_SIZE, padded_shapes=({\"sequence\": [None], \"other_features\": [None]},[None]))\n",
    "\n",
    "# pre fetch\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "unshuffled_train_dataset = unshuffled_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6gFg4W42X8q"
   },
   "outputs": [],
   "source": [
    "# dev decay as proposed in \"The Marginal Value of Adaptive Gradient Methods in Machine Learning\"\n",
    "\n",
    "class DevDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, performance_metric_name, gamma):\n",
    "        super(DevDecayCallback, self).__init__()\n",
    "        self.best_test_performance = -float('inf')\n",
    "        self.performance_metric_name = performance_metric_name\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def on_test_end(self, logs):\n",
    "        test_performance = logs[self.performance_metric_name]\n",
    "        if test_performance > self.best_test_performance:\n",
    "            self.best_test_performance = test_performance\n",
    "        else:\n",
    "            lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "            new_lr = lr * self.gamma\n",
    "            print(\"Lr decayed from {} to {}\".format(lr, new_lr))\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12mBm5B7BFY4"
   },
   "outputs": [],
   "source": [
    "# other features encoder start\n",
    "inp_other_features = tf.keras.Input(shape=[39],name=\"other_features\")\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(inp_other_features)\n",
    "x = tf.keras.layers.LayerNormalization()(x)\n",
    "other_features_encoder_end = tf.keras.layers.Dropout(0.6)(x)\n",
    "# other features encoder end\n",
    "\n",
    "# sequence encoder start\n",
    "inp_sequence = tf.keras.Input(shape=[None],name=\"sequence\")\n",
    "x = tf.keras.layers.Embedding(VOCAB_SIZE+2, 300, mask_zero=True, name=\"Embedding\")(inp_sequence)\n",
    "x = tf.keras.layers.LayerNormalization()(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "sequence_encoder_out = tf.keras.layers.Dropout(0.6)(x)\n",
    "# sequence encoder end\n",
    "\n",
    "x_concat = tf.keras.layers.concatenate([sequence_encoder_out,other_features_encoder_end])\n",
    "out = tf.keras.layers.Dense(NUM_LABELS,activation=\"softmax\")(x_concat)\n",
    "model = tf.keras.Model([inp_sequence,inp_other_features], out)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=5e-3, nesterov=True, momentum=0.8), loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.TopKCategoricalAccuracy(k=10,name=\"top_10_accuracy\")])\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"GEA_SWEM\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_top_10_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "dev_decay_callback = DevDecayCallback(performance_metric_name=\"accuracy\", gamma=0.95)\n",
    "\n",
    "history = model.fit(train_dataset,epochs=50,verbose=2,validation_data=validation_dataset,callbacks=[model_checkpoint_callback,dev_decay_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkkXwyDiMl5e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GEA_8_36.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}