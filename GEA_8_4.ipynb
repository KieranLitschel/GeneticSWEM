{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GEA_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDV0-KftQyYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95cd56d7-0344-4fbb-a33a-aa8e7aefcfc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK_yubJCjX1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "VALIDATION_PERCENT_SPLIT = 0\n",
        "TRAIN_SET_FRAC = 1 # fraction of training and validatipn set to use\n",
        "REBUILD_DATASET = False # whether to download pre-processed features, or pre-process from scratch (takes approx 20 mins extra if rebuilding from scratch)\n",
        "LOAD_TEST_SET = True # whether to load test set into memory\n",
        "BASES = [\"G\",\"A\",\"T\",\"C\",\"N\"]\n",
        "N = 8 # if you change this set rebuild dataset to True\n",
        "VOCAB = np.unique([''.join(permutation) for combination in itertools.combinations_with_replacement(BASES, r=N) for permutation in itertools.permutations(combination)])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4_VCHQAQ5aV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "530e80d4-2b11-446a-a837-037a4df640a8"
      },
      "source": [
        "# download the labels\n",
        "\n",
        "TRAIN_LABELS_URL = \"https://drivendata-prod.s3.amazonaws.com/data/63/public/train_labels.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200830%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200830T174802Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=7c3cbe31b74ef9aff3e958f0af7292336372bb2e36e5fd042942bf08ce2e95b7\"\n",
        "train_labels_file_path = tf.keras.utils.get_file(\"train_labels.csv\", TRAIN_LABELS_URL)\n",
        "train_labels_df = pd.read_csv(train_labels_file_path, index_col=\"sequence_id\")\n",
        "\n",
        "# preprocess the features\n",
        "\n",
        "encoder = tfds.features.text.SubwordTextEncoder(vocab_list=VOCAB)\n",
        "VOCAB_SIZE=len(VOCAB)\n",
        "\n",
        "if REBUILD_DATASET:\n",
        "    TRAIN_DATA_URL = \"https://drivendata-prod.s3.amazonaws.com/data/63/public/train_values.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200829T110926Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=097b0ed7c35d539666bdc3491076b140b8797bf32349f41d83737225de73b346\"\n",
        "    TEST_DATA_URL = \"https://drivendata-prod.s3.amazonaws.com/data/63/public/test_values.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200829T110926Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b9a06609a313b5519114f75c5106ed945f2fa31421872a85241ca802b031ec07\"\n",
        "\n",
        "    train_features_file_path = tf.keras.utils.get_file(\"train_features.csv\", TRAIN_DATA_URL)\n",
        "    test_features_file_path = tf.keras.utils.get_file(\"test_features.csv\", TEST_DATA_URL)\n",
        "\n",
        "    train_features_df = pd.read_csv(train_features_file_path, index_col=\"sequence_id\")\n",
        "    if LOAD_TEST_SET:\n",
        "        test_features_df = pd.read_csv(test_features_file_path, index_col=\"sequence_id\")\n",
        "\n",
        "    # encode sequence\n",
        "    def encode_sequence(features_file_path, encoder):\n",
        "        features_df = pd.read_csv(features_file_path, index_col=\"sequence_id\")\n",
        "        # if the len(sequence)%N != 0, we discard of the extra characters, we also encode each sequence of N characters seperately as SubwordTextEncoder computes overlapping encodings\n",
        "        features_df[\"sequence\"] = [[encoder.encode(sequence[i:i+N])[0] for i in range(0,len(sequence)-(N-1),N)] for sequence in tqdm(features_df[\"sequence\"])]\n",
        "        return features_df\n",
        "\n",
        "    train_features_df = encode_sequence(train_features_file_path, encoder)\n",
        "    if LOAD_TEST_SET:\n",
        "        test_features_df = encode_sequence(test_features_file_path, encoder)\n",
        "\n",
        "    # convert one-hot features to int\n",
        "    column_type_dict = {\"sequence\":object}\n",
        "    for column in train_features_df.columns[1:]:\n",
        "        column_type_dict[column] = np.int16\n",
        "    train_features_df = train_features_df.astype(column_type_dict)\n",
        "    train_features_df.to_pickle(\"base_{}_encoded_train_features_df.pickle\".format(N))\n",
        "    if LOAD_TEST_SET:\n",
        "        test_features_df = test_features_df.astype(column_type_dict)\n",
        "        test_features_df.to_pickle(\"base_{}_encoded_test_features_df.pickle\".format(N))\n",
        "else:\n",
        "    !gdown --id 1FwnBsK8EVp4U5U9zCsU505M-k2gcqyj-\n",
        "    !tar zxvf base_8_encoded.tar.gz -C .\n",
        "    train_features_df = pd.read_pickle(\"base_encoded/base_{}_encoded_train_features_df.pickle\".format(N))\n",
        "    if LOAD_TEST_SET:\n",
        "        test_features_df = pd.read_pickle(\"base_encoded/base_{}_encoded_test_features_df.pickle\".format(N))\n",
        "\n",
        "NUM_LABELS = len(train_labels_df.columns)\n",
        "\n",
        "# determine class weights\n",
        "\n",
        "train_labels_single_column = train_labels_df.dot(range(len(train_labels_df.columns))).astype(np.int16).values # converts one hot representation to single column\n",
        "labels_in_training_set = np.unique(train_labels_single_column)\n",
        "class_weights_list = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
        "                                                 labels_in_training_set,\n",
        "                                                 train_labels_single_column)\n",
        "class_weights = {class_no: weight for class_no, weight in zip(labels_in_training_set, class_weights_list)}\n",
        "\n",
        "# build validation set\n",
        "indexes = list(train_features_df.index)\n",
        "np.random.seed(26082020)\n",
        "np.random.shuffle(indexes)\n",
        "validation_samples = int(len(indexes) * VALIDATION_PERCENT_SPLIT)\n",
        "validation_indexes = indexes[:validation_samples]\n",
        "train_indexes = indexes[validation_samples:]\n",
        "\n",
        "validation_indexes = validation_indexes[:int(len(validation_indexes)*TRAIN_SET_FRAC)]\n",
        "train_indexes = train_indexes[:int(len(train_indexes)*TRAIN_SET_FRAC)]\n",
        "\n",
        "validation_features_df = train_features_df.loc[validation_indexes]\n",
        "validation_labels_df = train_labels_df.loc[validation_indexes]\n",
        "train_features_df = train_features_df.loc[train_indexes]\n",
        "train_labels_df = train_labels_df.loc[train_indexes]\n",
        "\n",
        "# the only way to get uneven lists into tf.data.Dataset is using ragged tensors, but padded\n",
        "# batch does not support ragged tensors, and we can not pad before training as we will run out\n",
        "# of memory, so we just convert the lists to binary and then convert them back to ints in the\n",
        "# pipeline\n",
        "\n",
        "train_features_df[\"sequence\"] = [pickle.dumps(sequence) for sequence in train_features_df[\"sequence\"]]\n",
        "validation_features_df[\"sequence\"] = [pickle.dumps(sequence) for sequence in validation_features_df[\"sequence\"]]\n",
        "if LOAD_TEST_SET:\n",
        "    test_features_df[\"sequence\"] = [pickle.dumps(sequence) for sequence in test_features_df[\"sequence\"]]\n",
        "\n",
        "# build datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":train_features_df[\"sequence\"].values,\"other_features\":train_features_df.drop(columns=\"sequence\").values},train_labels_df.values))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":validation_features_df[\"sequence\"].values,\"other_features\":validation_features_df.drop(columns=\"sequence\").values},validation_labels_df.values))\n",
        "if LOAD_TEST_SET:\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices({\"sequence\":test_features_df[\"sequence\"].values,\"other_features\":test_features_df.drop(columns=\"sequence\").values})\n",
        "\n",
        "# save unshufled train dataset for evaluation\n",
        "unshuffled_train_dataset = tf.data.Dataset.from_tensor_slices(({\"sequence\":train_features_df[\"sequence\"].values,\"other_features\":train_features_df.drop(columns=\"sequence\").values},train_labels_df.values))\n",
        "\n",
        "# shuffle results\n",
        "train_dataset = train_dataset.shuffle(BATCH_SIZE*2)\n",
        "\n",
        "# convert binary to ints\n",
        "\n",
        "def bin_to_int(sequence_tensor):\n",
        "    return [pickle.loads(sequence_tensor.numpy())]\n",
        "\n",
        "def tf_bin_to_int(*tensors):\n",
        "    if len(tensors) == 2:\n",
        "        features_dict, labels_tensor = tensors\n",
        "    else:\n",
        "        features_dict = tensors[0]\n",
        "    sequence_tensor = features_dict[\"sequence\"]\n",
        "    sequence_tensor = tf.py_function(bin_to_int, inp=[sequence_tensor], Tout=tf.int32)\n",
        "    sequence_tensor.set_shape([None])\n",
        "    features_dict[\"sequence\"] = sequence_tensor\n",
        "    if len(tensors) == 2:\n",
        "        tensors = (features_dict, labels_tensor)\n",
        "    else:\n",
        "        tensors = features_dict\n",
        "    return tensors\n",
        "\n",
        "train_dataset = train_dataset.map(tf_bin_to_int,\n",
        "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
        "unshuffled_train_dataset = unshuffled_train_dataset.map(tf_bin_to_int,\n",
        "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
        "validation_dataset = validation_dataset.map(tf_bin_to_int,\n",
        "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
        "if LOAD_TEST_SET:\n",
        "    test_dataset = test_dataset.map(tf_bin_to_int,\n",
        "                                  num_parallel_calls=multiprocessing.cpu_count())\n",
        "\n",
        "# pre fetch\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "unshuffled_train_dataset = unshuffled_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "if LOAD_TEST_SET:\n",
        "    test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# batch datasets\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "unshuffled_train_dataset = unshuffled_train_dataset.padded_batch(BATCH_SIZE)\n",
        "validation_dataset = validation_dataset.padded_batch(BATCH_SIZE)\n",
        "if LOAD_TEST_SET:\n",
        "    test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
        "\n",
        "# pre fetch\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "unshuffled_train_dataset = unshuffled_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "if LOAD_TEST_SET:\n",
        "    test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FwnBsK8EVp4U5U9zCsU505M-k2gcqyj-\n",
            "To: /content/base_8_encoded.tar.gz\n",
            "92.6MB [00:00, 252MB/s]\n",
            "base_encoded/\n",
            "base_encoded/base_8_encoded_test_features_df.pickle\n",
            "base_encoded/base_8_encoded_train_features_df.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y59Ij_85WrEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _top_10_accuracy_scorer(y_true, y_pred):\n",
        "    # get the indices for top 10 predictions for each row; these are the last ten in each row\n",
        "    # Note: We use argpartition, which is O(n), vs argsort, which uses the quicksort algorithm \n",
        "    # by default and is O(n^2) in the worst case. We can do this because we only need the top ten\n",
        "    # partitioned, not in sorted order.\n",
        "    # Documentation: https://numpy.org/doc/1.18/reference/generated/numpy.argpartition.html\n",
        "    top10_idx = np.argpartition(y_pred, -10, axis=1)[:, -10:]\n",
        "    \n",
        "    # set top 10 indexes to 1's, the rest 0\n",
        "    top_10_identity = np.zeros(y_pred.shape)\n",
        "    for sample_no, top_10 in enumerate(top10_idx):\n",
        "        top_10_identity[sample_no][top_10] = 1\n",
        "\n",
        "    # determine the number correct\n",
        "    top_10_correct = np.sum(top_10_identity*y_true,axis=1)\n",
        "    \n",
        "    # take the mean\n",
        "    top_10_accuracy = np.mean(top_10_correct)\n",
        " \n",
        "    return top_10_accuracy\n",
        "\n",
        "def top10_accuracy_scorer(model, dataset, ground_truths):\n",
        "    \"\"\"A custom scorer that evaluates a model on whether the correct label is in \n",
        "    the top 10 most probable predictions.\n",
        "\n",
        "    Args:\n",
        "        model (tf.model): The tf model that should be evaluated.\n",
        "        dataset (tf.data.Dataset): The validation data.\n",
        "        ground_truths (numpy array): The one-hot-encoded ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model as defined by the proportion of predictions\n",
        "               in which the correct label was in the top 10. Higher is better.\n",
        "    \"\"\"\n",
        "    # predict the probabilities across all possible labels for rows in our training set\n",
        "    probas = model.predict(dataset)\n",
        "    \n",
        "    return _top_10_accuracy_scorer(ground_truths, probas)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZItpsCVbxDJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9b167777-3656-40a1-d9ce-48c837e42a94"
      },
      "source": [
        "# sequence encoder start\n",
        "inp_sequence = tf.keras.Input(shape=[None],name=\"sequence\")\n",
        "x = tf.keras.layers.Embedding(VOCAB_SIZE+1, 16, mask_zero=True, name=\"Embedding\",embeddings_regularizer=tf.keras.regularizers.l2(1e-5))(inp_sequence)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32,return_sequences=True,dropout=0.25))(x)\n",
        "# inception v2 block start, no Max1D as GlobalMaxPool makes it redundant\n",
        "inception_conv_1 = tf.keras.layers.Conv1D(32,1,padding=\"same\",name=\"inception_conv_1\")(x)\n",
        "inception_conv_3 = tf.keras.layers.Conv1D(32,3,padding=\"same\",name=\"inception_conv_3\")(x)\n",
        "inception_conv_5 = tf.keras.layers.Conv1D(32,3,padding=\"same\",name=\"inception_conv_5_2\")(tf.keras.layers.Conv1D(32,3,padding=\"same\",name=\"inception_conv_5_1\")(x))\n",
        "inception_concat = tf.keras.layers.concatenate([inception_conv_1,inception_conv_3,inception_conv_5])\n",
        "# inception v2 block end\n",
        "sequence_encoder_out = tf.keras.layers.GlobalMaxPool1D()(inception_concat)\n",
        "# sequence encoder end\n",
        "\n",
        "# other features encoder start\n",
        "inp_other_features = tf.keras.Input(shape=[39],name=\"other_features\")\n",
        "other_features_encoder_end = tf.keras.layers.Dense(64,activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l1(1e-6))(inp_other_features)\n",
        "# other features encoder end\n",
        "\n",
        "x = tf.keras.layers.concatenate([sequence_encoder_out,other_features_encoder_end])\n",
        "x = tf.keras.layers.Dense(128,activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2())(x)\n",
        "out = tf.keras.layers.Dense(NUM_LABELS,activation=\"softmax\")(x)\n",
        "model = tf.keras.Model([inp_sequence,inp_other_features], out)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4*10**(9/4)), loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.TopKCategoricalAccuracy(k=10,name=\"top_10_accuracy\")])\n",
        "\n",
        "history = model.fit(train_dataset,epochs=10,verbose=1)#,validation_data=validation_dataset)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "247/247 [==============================] - 85s 343ms/step - loss: 4.2424 - accuracy: 0.3459 - top_10_accuracy: 0.5629\n",
            "Epoch 2/10\n",
            "247/247 [==============================] - 85s 343ms/step - loss: 3.1842 - accuracy: 0.4680 - top_10_accuracy: 0.7176\n",
            "Epoch 3/10\n",
            "247/247 [==============================] - 84s 339ms/step - loss: 2.8831 - accuracy: 0.5193 - top_10_accuracy: 0.7706\n",
            "Epoch 4/10\n",
            "247/247 [==============================] - 84s 340ms/step - loss: 2.7375 - accuracy: 0.5508 - top_10_accuracy: 0.8021\n",
            "Epoch 5/10\n",
            "247/247 [==============================] - 84s 340ms/step - loss: 2.6384 - accuracy: 0.5765 - top_10_accuracy: 0.8230\n",
            "Epoch 6/10\n",
            "247/247 [==============================] - 84s 339ms/step - loss: 2.5714 - accuracy: 0.5959 - top_10_accuracy: 0.8415\n",
            "Epoch 7/10\n",
            "247/247 [==============================] - 84s 338ms/step - loss: 2.5160 - accuracy: 0.6138 - top_10_accuracy: 0.8554\n",
            "Epoch 8/10\n",
            "247/247 [==============================] - 84s 339ms/step - loss: 2.4688 - accuracy: 0.6300 - top_10_accuracy: 0.8665\n",
            "Epoch 9/10\n",
            "247/247 [==============================] - 84s 341ms/step - loss: 2.4139 - accuracy: 0.6399 - top_10_accuracy: 0.8776\n",
            "Epoch 10/10\n",
            "247/247 [==============================] - 83s 338ms/step - loss: 2.3864 - accuracy: 0.6536 - top_10_accuracy: 0.8858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NonULkn296gL",
        "colab_type": "text"
      },
      "source": [
        "No analysis as validation set was used for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkfBs8zJCtZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "dcfa36fd-1d3f-44b3-abee-bbe4eba7c123"
      },
      "source": [
        "model.save(\"GE_8_4\")\n",
        "!tar -czvf GE_8_4.tar.gz GE_8_4/"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: GE_8_4/assets\n",
            "GE_8_4/\n",
            "GE_8_4/saved_model.pb\n",
            "GE_8_4/assets/\n",
            "GE_8_4/variables/\n",
            "GE_8_4/variables/variables.index\n",
            "GE_8_4/variables/variables.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DysEsxgRn1RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict test set and save in submission format\n",
        "test_prob = model.predict(test_dataset)\n",
        "test_predicted_labels = pd.DataFrame(test_prob,columns=train_labels_df.columns,index=test_features_df.index)\n",
        "test_predicted_labels.to_csv(\"GE_8_4_test_predicted_labels.csv\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFcUO5BL-Vvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}